{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-19 13:17:08,110\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-03-19 13:17:09,946\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 13:41:34,794\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-03-19 13:41:34,809\tWARNING callback.py:136 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-19 13:50:43</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:08.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.9/31.1 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=10<br>Bracket: Iter 8.000: -1.2337173930287362 | Iter 4.000: -1.3028179175496102 | Iter 2.000: -1.4053144740819932 | Iter 1.000: -1.8881408295869828<br>Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_dfcab_00000</td><td>TERMINATED</td><td>10.30.68.0:24633</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.00255805 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        135.817 </td><td style=\"text-align: right;\">1.97046</td><td style=\"text-align: right;\">    0.2164</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00001</td><td>TERMINATED</td><td>10.30.68.0:24628</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  16</td><td style=\"text-align: right;\">0.00512428 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        531.86  </td><td style=\"text-align: right;\">1.35922</td><td style=\"text-align: right;\">    0.5311</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00002</td><td>TERMINATED</td><td>10.30.68.0:24630</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.000713072</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        541.552 </td><td style=\"text-align: right;\">1.0907 </td><td style=\"text-align: right;\">    0.6292</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00003</td><td>TERMINATED</td><td>10.30.68.0:24634</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">   8</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.0219797  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.2008</td><td style=\"text-align: right;\">2.31342</td><td style=\"text-align: right;\">    0.0994</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00004</td><td>TERMINATED</td><td>10.30.68.0:24635</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">0.0200457  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        401.945 </td><td style=\"text-align: right;\">2.30442</td><td style=\"text-align: right;\">    0.1004</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00005</td><td>TERMINATED</td><td>10.30.68.0:24631</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.000984582</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        408.214 </td><td style=\"text-align: right;\">1.13432</td><td style=\"text-align: right;\">    0.6022</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00006</td><td>TERMINATED</td><td>10.30.68.0:24629</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">0.0337012  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        256.276 </td><td style=\"text-align: right;\">2.32175</td><td style=\"text-align: right;\">    0.0992</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00007</td><td>TERMINATED</td><td>10.30.68.0:24632</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">0.0222537  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        252.221 </td><td style=\"text-align: right;\">2.31544</td><td style=\"text-align: right;\">    0.1015</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00008</td><td>TERMINATED</td><td>10.30.68.0:27955</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">  16</td><td style=\"text-align: right;\">  16</td><td style=\"text-align: right;\">0.00402611 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        228.466 </td><td style=\"text-align: right;\">2.27   </td><td style=\"text-align: right;\">    0.1198</td></tr>\n",
       "<tr><td>train_cifar_dfcab_00009</td><td>TERMINATED</td><td>10.30.68.0:29486</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">0.00384206 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        390.966 </td><td style=\"text-align: right;\">1.26165</td><td style=\"text-align: right;\">    0.5841</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">   loss</th><th>should_checkpoint  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_dfcab_00000</td><td style=\"text-align: right;\">    0.2164</td><td style=\"text-align: right;\">1.97046</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00001</td><td style=\"text-align: right;\">    0.5311</td><td style=\"text-align: right;\">1.35922</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00002</td><td style=\"text-align: right;\">    0.6292</td><td style=\"text-align: right;\">1.0907 </td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00003</td><td style=\"text-align: right;\">    0.0994</td><td style=\"text-align: right;\">2.31342</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00004</td><td style=\"text-align: right;\">    0.1004</td><td style=\"text-align: right;\">2.30442</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00005</td><td style=\"text-align: right;\">    0.6022</td><td style=\"text-align: right;\">1.13432</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00006</td><td style=\"text-align: right;\">    0.0992</td><td style=\"text-align: right;\">2.32175</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00007</td><td style=\"text-align: right;\">    0.1015</td><td style=\"text-align: right;\">2.31544</td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00008</td><td style=\"text-align: right;\">    0.1198</td><td style=\"text-align: right;\">2.27   </td><td>True               </td></tr>\n",
       "<tr><td>train_cifar_dfcab_00009</td><td style=\"text-align: right;\">    0.5841</td><td style=\"text-align: right;\">1.26165</td><td>True               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 13:50:43,336\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/kuo1st/ray_results/train_cifar_2025-03-19_13-41-34' in 0.0228s.\n",
      "2025-03-19 13:50:43,345\tINFO tune.py:1041 -- Total run time: 548.55 seconds (548.49 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 256, 'l2': 256, 'lr': 0.0007130723004703986, 'batch_size': 8}\n",
      "Best trial final validation loss: 1.0907001942873\n",
      "Best trial final validation accuracy: 0.6292\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Best trial test set accuracy: 0.6397\n"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    result = tune.run(\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "        test_acc = test_accuracy(best_trained_model, device)\n",
    "        print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
