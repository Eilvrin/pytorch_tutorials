{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from mlflow.types import Schema, TensorSpec\n",
    "from mlflow.models import ModelSignature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:11<00:00, 2.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 388kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:02<00:00, 1.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 40.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, metrics_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        accuracy = metrics_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch\n",
    "            mlflow.log_metric(\"loss\", f\"{loss:3f}\", step=(batch // 100))\n",
    "            mlflow.log_metric(\"accuracy\", f\"{accuracy:3f}\", step=(batch // 100))\n",
    "            print(f\"loss: {loss:3f} accuracy: {accuracy:3f} [{current} / {len(dataloader)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------------\n",
      "loss: 2.295373 accuracy: 0.156250 [0 / 938]\n",
      "loss: 2.290316 accuracy: 0.218750 [100 / 938]\n",
      "loss: 2.262481 accuracy: 0.375000 [200 / 938]\n",
      "loss: 2.261474 accuracy: 0.359375 [300 / 938]\n",
      "loss: 2.244850 accuracy: 0.375000 [400 / 938]\n",
      "loss: 2.214631 accuracy: 0.375000 [500 / 938]\n",
      "loss: 2.215840 accuracy: 0.281250 [600 / 938]\n",
      "loss: 2.182413 accuracy: 0.406250 [700 / 938]\n",
      "loss: 2.180620 accuracy: 0.390625 [800 / 938]\n",
      "loss: 2.146815 accuracy: 0.453125 [900 / 938]\n",
      "Epoch 2\n",
      "----------------------------\n",
      "loss: 2.154326 accuracy: 0.390625 [0 / 938]\n",
      "loss: 2.150261 accuracy: 0.375000 [100 / 938]\n",
      "loss: 2.077930 accuracy: 0.484375 [200 / 938]\n",
      "loss: 2.104153 accuracy: 0.484375 [300 / 938]\n",
      "loss: 2.043926 accuracy: 0.484375 [400 / 938]\n",
      "loss: 1.985053 accuracy: 0.531250 [500 / 938]\n",
      "loss: 2.010692 accuracy: 0.406250 [600 / 938]\n",
      "loss: 1.930540 accuracy: 0.468750 [700 / 938]\n",
      "loss: 1.937635 accuracy: 0.468750 [800 / 938]\n",
      "loss: 1.865651 accuracy: 0.546875 [900 / 938]\n",
      "Epoch 3\n",
      "----------------------------\n",
      "loss: 1.894872 accuracy: 0.515625 [0 / 938]\n",
      "loss: 1.872697 accuracy: 0.484375 [100 / 938]\n",
      "loss: 1.740487 accuracy: 0.640625 [200 / 938]\n",
      "loss: 1.792947 accuracy: 0.515625 [300 / 938]\n",
      "loss: 1.674822 accuracy: 0.562500 [400 / 938]\n",
      "loss: 1.636724 accuracy: 0.656250 [500 / 938]\n",
      "loss: 1.652741 accuracy: 0.578125 [600 / 938]\n",
      "loss: 1.566264 accuracy: 0.578125 [700 / 938]\n",
      "loss: 1.588451 accuracy: 0.531250 [800 / 938]\n",
      "loss: 1.484478 accuracy: 0.656250 [900 / 938]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02098488, -0.73580617,  0.64805454, -0.5261527 ,  0.52522826,\n",
       "        -0.4032299 ,  0.36929798, -0.52065235,  0.69928557, -0.00839979]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "metric_fn = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    params = {\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"loss_function\": loss_fn.__class__.__name__,\n",
    "        \"metric_function\": metric_fn.__class__.__name__,\n",
    "        \"optimizer\": \"SGD\",\n",
    "    }\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    with open(\"model_summary.txt\", \"w\") as f:\n",
    "        f.write(str(summary(model)))\n",
    "    mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n----------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, metric_fn, optimizer)\n",
    "\n",
    "    input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 28, 28))])\n",
    "    output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 10))])\n",
    "    signature = ModelSignature(inputs = input_schema, outputs = output_schema)\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "logged_model = f\"runs:/{run.info.run_id}/model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "loaded_model.predict(np.random.uniform(size=[1, 28, 28]).astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
