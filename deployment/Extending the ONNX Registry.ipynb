{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "import onnxscript\n",
    "from onnxscript import opset18  # opset 18 is the latest (and only) supported version for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom operators with existing ONNX Runtime support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "/home/kuo1st/.local/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/kuo1st/.local/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::gelu.default is supported by ONNX registry:     True\n"
     ]
    }
   ],
   "source": [
    "onnx_registry = torch.onnx.OnnxRegistry()\n",
    "print(f\"aten::gelu.default is supported by ONNX registry: \\\n",
    "    {onnx_registry.is_registered_op(namespace='aten', op_name='gelu', overload='default')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Gelu' is not a known op in 'com.microsoft'\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class CustomGelu(torch.nn.Module):\n",
    "    def forward(self, input_x):\n",
    "        return torch.ops.aten.gelu(input_x)\n",
    "\n",
    "# com.microsoft is an official ONNX Runtime namspace\n",
    "custom_ort = onnxscript.values.Opset(domain=\"com.microsoft\", version=1)\n",
    "\n",
    "# NOTE: The function signature must match the signature of the unsupported ATen operator.\n",
    "# https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml\n",
    "# NOTE: All attributes must be annotated with type hints.\n",
    "@onnxscript.script(custom_ort)\n",
    "def custom_aten_gelu(input_x, approximate: str = \"none\"):\n",
    "    # We know com.microsoft::Gelu is supported by ONNX Runtime\n",
    "    # It's only not supported by ONNX\n",
    "    return custom_ort.Gelu(input_x)\n",
    "\n",
    "\n",
    "onnx_registry = torch.onnx.OnnxRegistry()\n",
    "onnx_registry.register_op(\n",
    "    namespace=\"aten\", op_name=\"gelu\", overload=\"default\", function=custom_aten_gelu)\n",
    "export_options = torch.onnx.ExportOptions(onnx_registry=onnx_registry)\n",
    "\n",
    "aten_gelu_model = CustomGelu()\n",
    "input_gelu_x = torch.randn(3, 3)\n",
    "\n",
    "onnx_program = torch.onnx.dynamo_export(\n",
    "    aten_gelu_model, input_gelu_x, export_options=export_options\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph node domain is the custom domain we registered\n",
    "assert onnx_program.model_proto.graph.node[0].domain == \"com.microsoft\"\n",
    "# graph node name is the function name\n",
    "assert onnx_program.model_proto.graph.node[0].op_type == \"Gelu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"./custom_gelu_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"./custom_gelu_model.onnx\", providers=['CPUExecutionProvider']\n",
    "    )\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnx_input = [input_gelu_x]\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)[0]\n",
    "\n",
    "torch_outputs = aten_gelu_model(input_gelu_x)\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom operators without ONNX Runtime support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is a beta feature in PyTorch, and is subject to change.\n",
    "from torch._custom_op import impl as custom_op\n",
    "\n",
    "@custom_op.custom_op(\"mylibrary::addandround_op\")\n",
    "def addandround_op(tensor_x: torch.Tensor) -> torch.Tensor:\n",
    "    ...\n",
    "\n",
    "@addandround_op.impl_abstract()\n",
    "def addandround_op_impl_abstract(tensor_x):\n",
    "    return torch.empty_like(tensor_x)\n",
    "\n",
    "@addandround_op.impl(\"cpu\")\n",
    "def addandround_op_impl(tensor_x):\n",
    "    return torch.round(tensor_x + tensor_x)  # add x to itself, and round the result\n",
    "\n",
    "torch._dynamo.allow_in_graph(addandround_op)\n",
    "\n",
    "class CustomFoo(torch.nn.Module):\n",
    "    def forward(self, tensor_x):\n",
    "        return addandround_op(tensor_x)\n",
    "\n",
    "input_addandround_x = torch.randn(3)\n",
    "custom_addandround_model = CustomFoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'CustomOpOne' is not a known op in 'test.customop'\n",
      "'CustomOpTwo' is not a known op in 'test.customop'\n",
      "custom_addandround: Already defined.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "custom_opset = onnxscript.values.Opset(domain=\"test.customop\", version=1)\n",
    "\n",
    "# NOTE: The function signature must match the signature of the unsupported ATen operator.\n",
    "# https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml\n",
    "# NOTE: All attributes must be annotated with type hints.\n",
    "@onnxscript.script(custom_opset)\n",
    "def custom_addandround(input_x):\n",
    "    # The same as opset18.Add(x, x)\n",
    "    add_x = custom_opset.CustomOpOne(input_x, input_x)\n",
    "    # The same as opset18.Round(x, x)\n",
    "    round_x = custom_opset.CustomOpTwo(add_x)\n",
    "    # Cast to FLOAT to match the ONNX type\n",
    "    return opset18.Cast(round_x, to=1)\n",
    "\n",
    "\n",
    "onnx_registry = torch.onnx.OnnxRegistry()\n",
    "onnx_registry.register_op(\n",
    "    namespace=\"mylibrary\", op_name=\"addandround_op\", overload=\"default\", function=custom_addandround\n",
    "    )\n",
    "\n",
    "export_options = torch.onnx.ExportOptions(onnx_registry=onnx_registry)\n",
    "onnx_program = torch.onnx.dynamo_export(\n",
    "    custom_addandround_model, input_addandround_x, export_options=export_options\n",
    "    )\n",
    "onnx_program.save(\"./custom_addandround_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onnx_program.model_proto.graph.node[0].domain == \"test.customop\"\n",
    "assert onnx_program.model_proto.graph.node[0].op_type == \"custom_addandround\"\n",
    "assert onnx_program.model_proto.functions[0].node[0].domain == \"test.customop\"\n",
    "assert onnx_program.model_proto.functions[0].node[0].op_type == \"CustomOpOne\"\n",
    "assert onnx_program.model_proto.functions[0].node[1].domain == \"test.customop\"\n",
    "assert onnx_program.model_proto.functions[0].node[1].op_type == \"CustomOpTwo\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
