{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79bb2ac-a7ee-4f17-abfb-3c7b2777161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec97e962-eeac-447d-a628-c489912fe378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pullover  Trouser  Dress  Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa389bb-e0c1-4b76-b408-95f76b3c69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43bab21-94f6-4d95-a28f-680bcfb522d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1749, 0.4437, 0.2424, 0.1122, 0.7050, 0.4027, 0.0469, 0.0558, 0.9689,\n",
      "         0.4330],\n",
      "        [0.1054, 0.3152, 0.5272, 0.3847, 0.8331, 0.9104, 0.0525, 0.7419, 0.2985,\n",
      "         0.5146],\n",
      "        [0.4508, 0.3589, 0.9964, 0.9867, 0.5846, 0.1961, 0.5473, 0.8295, 0.8542,\n",
      "         0.0727],\n",
      "        [0.5257, 0.1606, 0.3799, 0.9851, 0.5323, 0.9258, 0.4116, 0.3148, 0.5722,\n",
      "         0.6150]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.1675868034362793\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27ef863-02c9-4192-9633-08dfb0346dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1994e90a-aae5-4f94-baa9-987b301a15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766cd1d5-c217-4e2f-8c45-3a24de3e1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.741730257343501\n",
      "  batch 2000 loss: 0.8725179031630979\n",
      "  batch 3000 loss: 0.7098379850704223\n",
      "  batch 4000 loss: 0.6261381238992326\n",
      "  batch 5000 loss: 0.5918821355183609\n",
      "  batch 6000 loss: 0.5392108988692053\n",
      "  batch 7000 loss: 0.5184039456816391\n",
      "  batch 8000 loss: 0.48488431853292424\n",
      "  batch 9000 loss: 0.5006056426795549\n",
      "  batch 10000 loss: 0.468348097525537\n",
      "  batch 11000 loss: 0.47426317454106176\n",
      "  batch 12000 loss: 0.4381882663205615\n",
      "  batch 13000 loss: 0.44570477814436893\n",
      "  batch 14000 loss: 0.42261143127596007\n",
      "  batch 15000 loss: 0.395508100749008\n",
      "LOSS train 0.395508100749008 valid 0.41065898537635803\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.41760487597994506\n",
      "  batch 2000 loss: 0.3988827546946995\n",
      "  batch 3000 loss: 0.4045096518688806\n",
      "  batch 4000 loss: 0.40281068783770024\n",
      "  batch 5000 loss: 0.37987454457547576\n",
      "  batch 6000 loss: 0.36947250997897935\n",
      "  batch 7000 loss: 0.3703134588355315\n",
      "  batch 8000 loss: 0.3734056976177089\n",
      "  batch 9000 loss: 0.3630569619382586\n",
      "  batch 10000 loss: 0.36590741484833417\n",
      "  batch 11000 loss: 0.37155447105399797\n",
      "  batch 12000 loss: 0.35982315697649025\n",
      "  batch 13000 loss: 0.34949306190491186\n",
      "  batch 14000 loss: 0.3764596962806245\n",
      "  batch 15000 loss: 0.3422209314397478\n",
      "LOSS train 0.3422209314397478 valid 0.37209567427635193\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.35008946073061087\n",
      "  batch 2000 loss: 0.33476130101512536\n",
      "  batch 3000 loss: 0.32005428425954596\n",
      "  batch 4000 loss: 0.3321009720401635\n",
      "  batch 5000 loss: 0.3364991125296801\n",
      "  batch 6000 loss: 0.3292271628335875\n",
      "  batch 7000 loss: 0.3387962473970256\n",
      "  batch 8000 loss: 0.3226904864565004\n",
      "  batch 9000 loss: 0.3587794336093357\n",
      "  batch 10000 loss: 0.32066889776385504\n",
      "  batch 11000 loss: 0.3195037652058236\n",
      "  batch 12000 loss: 0.33461681622822653\n",
      "  batch 13000 loss: 0.32649748723542144\n",
      "  batch 14000 loss: 0.3242503374250518\n",
      "  batch 15000 loss: 0.32521423428350044\n",
      "LOSS train 0.32521423428350044 valid 0.34453433752059937\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.31374766991527575\n",
      "  batch 2000 loss: 0.3136490709393183\n",
      "  batch 3000 loss: 0.28173057208289176\n",
      "  batch 4000 loss: 0.3035731901031795\n",
      "  batch 5000 loss: 0.29883321382068245\n",
      "  batch 6000 loss: 0.3215792790557898\n",
      "  batch 7000 loss: 0.31005277855558777\n",
      "  batch 8000 loss: 0.2936512464065454\n",
      "  batch 9000 loss: 0.3062841159326781\n",
      "  batch 10000 loss: 0.29420551954046825\n",
      "  batch 11000 loss: 0.29997202741771617\n",
      "  batch 12000 loss: 0.307599321679867\n",
      "  batch 13000 loss: 0.2813461579736322\n",
      "  batch 14000 loss: 0.32644202609173956\n",
      "  batch 15000 loss: 0.304616257098678\n",
      "LOSS train 0.304616257098678 valid 0.3219887316226959\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.2817331350026216\n",
      "  batch 2000 loss: 0.27754316335076146\n",
      "  batch 3000 loss: 0.31023828713930746\n",
      "  batch 4000 loss: 0.27536486290182255\n",
      "  batch 5000 loss: 0.28122594918595134\n",
      "  batch 6000 loss: 0.27398204992290265\n",
      "  batch 7000 loss: 0.2741256863032795\n",
      "  batch 8000 loss: 0.30012513965248944\n",
      "  batch 9000 loss: 0.28648581625470115\n",
      "  batch 10000 loss: 0.2811873751289422\n",
      "  batch 11000 loss: 0.29741377502105026\n",
      "  batch 12000 loss: 0.273443549278607\n",
      "  batch 13000 loss: 0.2748702715683066\n",
      "  batch 14000 loss: 0.2766931094184838\n",
      "  batch 15000 loss: 0.2823062054260208\n",
      "LOSS train 0.2823062054260208 valid 0.3232002258300781\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.25351012165302383\n",
      "  batch 2000 loss: 0.2607167186780662\n",
      "  batch 3000 loss: 0.2672660039085495\n",
      "  batch 4000 loss: 0.27917809147513617\n",
      "  batch 5000 loss: 0.2611205758357755\n",
      "  batch 6000 loss: 0.27159620083927305\n",
      "  batch 7000 loss: 0.26788715758065335\n",
      "  batch 8000 loss: 0.27151161702825266\n",
      "  batch 9000 loss: 0.252639380712033\n",
      "  batch 10000 loss: 0.2618822650465627\n",
      "  batch 11000 loss: 0.2887979585076928\n",
      "  batch 12000 loss: 0.29270953508232106\n",
      "  batch 13000 loss: 0.268500233681616\n",
      "  batch 14000 loss: 0.27402847186700685\n",
      "  batch 15000 loss: 0.2728986192298953\n",
      "LOSS train 0.2728986192298953 valid 0.3496865928173065\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 0.2632275303515198\n",
      "  batch 2000 loss: 0.2649242284458724\n",
      "  batch 3000 loss: 0.23983256866161173\n",
      "  batch 4000 loss: 0.2572940057839678\n",
      "  batch 5000 loss: 0.2551732088032059\n",
      "  batch 6000 loss: 0.25433530321664877\n",
      "  batch 7000 loss: 0.246232405869317\n",
      "  batch 8000 loss: 0.24622333717613745\n",
      "  batch 9000 loss: 0.2653131687570085\n",
      "  batch 10000 loss: 0.2720873246068295\n",
      "  batch 11000 loss: 0.2626948836589763\n",
      "  batch 12000 loss: 0.24660005201574473\n",
      "  batch 13000 loss: 0.2636811554236847\n",
      "  batch 14000 loss: 0.24741528459845494\n",
      "  batch 15000 loss: 0.26738607875763953\n",
      "LOSS train 0.26738607875763953 valid 0.2965009808540344\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 0.22004150375867265\n",
      "  batch 2000 loss: 0.25930813664821745\n",
      "  batch 3000 loss: 0.2406711824698341\n",
      "  batch 4000 loss: 0.2507944694481047\n",
      "  batch 5000 loss: 0.2606859927097066\n",
      "  batch 6000 loss: 0.2422508872322428\n",
      "  batch 7000 loss: 0.22335484948612974\n",
      "  batch 8000 loss: 0.24418733729900213\n",
      "  batch 9000 loss: 0.2559578769246359\n",
      "  batch 10000 loss: 0.26132694692857333\n",
      "  batch 11000 loss: 0.25708542476520596\n",
      "  batch 12000 loss: 0.2454095930219337\n",
      "  batch 13000 loss: 0.22911127998359188\n",
      "  batch 14000 loss: 0.24237495219258848\n",
      "  batch 15000 loss: 0.24678239973332347\n",
      "LOSS train 0.24678239973332347 valid 0.332699179649353\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 0.21516336028718933\n",
      "  batch 2000 loss: 0.2123011282408196\n",
      "  batch 3000 loss: 0.2532720134463507\n",
      "  batch 4000 loss: 0.22006360523555826\n",
      "  batch 5000 loss: 0.22032432741216174\n",
      "  batch 6000 loss: 0.23822978180054896\n",
      "  batch 7000 loss: 0.2517669121594154\n",
      "  batch 8000 loss: 0.23121367691699346\n",
      "  batch 9000 loss: 0.23499310390035588\n",
      "  batch 10000 loss: 0.2372568058724464\n",
      "  batch 11000 loss: 0.24120783538441537\n",
      "  batch 12000 loss: 0.22796239751842018\n",
      "  batch 13000 loss: 0.24873406599718828\n",
      "  batch 14000 loss: 0.25180699196736167\n",
      "  batch 15000 loss: 0.24518935905786565\n",
      "LOSS train 0.24518935905786565 valid 0.30637016892433167\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 0.2022906121515591\n",
      "  batch 2000 loss: 0.23150880956960646\n",
      "  batch 3000 loss: 0.2232650221209205\n",
      "  batch 4000 loss: 0.21542905100223103\n",
      "  batch 5000 loss: 0.22994362769292911\n",
      "  batch 6000 loss: 0.22706439128222428\n",
      "  batch 7000 loss: 0.2204247811762375\n",
      "  batch 8000 loss: 0.21257415158869555\n",
      "  batch 9000 loss: 0.2327876397646974\n",
      "  batch 10000 loss: 0.22926555175420157\n",
      "  batch 11000 loss: 0.22293022389891484\n",
      "  batch 12000 loss: 0.23536437217947695\n",
      "  batch 13000 loss: 0.23871857373830993\n",
      "  batch 14000 loss: 0.23891397850736212\n",
      "  batch 15000 loss: 0.22527023220186537\n",
      "LOSS train 0.22527023220186537 valid 0.3165302872657776\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b10ef-6b15-4ce6-a266-8e273690215c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
